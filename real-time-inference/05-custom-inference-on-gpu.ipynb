{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"europe-west4\"\n",
    "PROJECT_ID = \"huggingface-cloud\"\n",
    "REPOSITORY = \"custom-inference-gpu\"\n",
    "IMAGE = \"huggingface-pipeline-gpu\"\n",
    "TAG = \"py310-cu12.3-torch-2.2.0-transformers-4.38.1\"\n",
    "BUCKET_NAME = \"huggingface-cloud\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}/bart-large-mnli/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/facebook/bart-large-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd bart-large-mnli/ && tar zcvf model.tar.gz --exclude flax_model.msgpack --exclude pytorch_model.bin --exclude rust_model.ot * && mv model.tar.gz ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "!gcloud config set storage/parallel_composite_upload_enabled True\n",
    "!gcloud storage cp model.tar.gz $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "!gcloud storage ls --recursive gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir huggingface_predictor_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile huggingface_predictor_gpu/predictor.py\n",
    "import os\n",
    "import logging\n",
    "import tarfile\n",
    "from typing import Any, Dict\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "class HuggingFacePredictor(Predictor):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def load(self, artifacts_uri: str) -> None:\n",
    "        \"\"\"Loads the preprocessor and model artifacts.\"\"\"\n",
    "        logger.debug(f\"Downloading artifacts from {artifacts_uri}\")\n",
    "        prediction_utils.download_model_artifacts(artifacts_uri)\n",
    "        logger.debug(\"Artifacts successfully downloaded!\")\n",
    "        os.makedirs(\"./model\", exist_ok=True)\n",
    "        with tarfile.open(\"model.tar.gz\", \"r:gz\") as tar:\n",
    "            tar.extractall(path=\"./model\")\n",
    "        logger.debug(f\"HF_TASK value is {os.getenv('HF_TASK')}\")\n",
    "        self._pipeline = pipeline(os.getenv(\"HF_TASK\", None), model=\"./model\", device_map=\"auto\")\n",
    "        logger.debug(\"`pipeline` successfully loaded!\")\n",
    "        logger.debug(f\"`pipeline` is using device={self._pipeline.device}\")\n",
    "\n",
    "    def predict(self, instances: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        return self._pipeline(**instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile huggingface_predictor_gpu/requirements.txt\n",
    "transformers==4.38.1\n",
    "accelerate==0.27.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "\n",
    "from huggingface_predictor_gpu.predictor import HuggingFacePredictor\n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    \"huggingface_predictor_gpu\",\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}:{TAG}\",\n",
    "    predictor=HuggingFacePredictor,\n",
    "    requirements_path=\"huggingface_predictor_gpu/requirements.txt\",\n",
    "    # base_image=\"--platform=linux/amd64 nvcr.io/nvidia/pytorch:23.11-py3 AS build\",\n",
    "    base_image=\"--platform=linux/amd64 alvarobartt/torch-gpu:py310-cu12.3-torch-2.2.0 AS build\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create custom-inference-gpu --repository-format=docker --location={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.push_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d682d8388ec"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2738154345d5"
   },
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"bart-large-mnli\",\n",
    "    artifact_uri=\"gs://huggingface-cloud/bart-large-mnli\",\n",
    "    serving_container_image_uri=local_model.get_serving_container_spec().image_uri,\n",
    "    serving_container_environment_variables={\n",
    "        \"HF_TASK\": \"zero-shot-classification\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62cf66498a28"
   },
   "outputs": [],
   "source": [
    "from contexttimer import Timer\n",
    "\n",
    "with Timer() as timer:\n",
    "    endpoint = model.deploy(\n",
    "        machine_type=\"g2-standard-4\",\n",
    "        accelerator_type=\"NVIDIA_L4\",\n",
    "        accelerator_count=1,\n",
    "    )\n",
    "print(f\"Time to deploy `{model.display_name}` into endpoint `{endpoint.resource_name}` was {timer.elapsed}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from google.api import httpbody_pb2\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "prediction_client = aiplatform_v1.PredictionServiceClient(\n",
    "    client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
    ")\n",
    "\n",
    "data = {\n",
    "    \"sequences\": \"Football is a sport\",\n",
    "    \"candidate_labels\": [\"soccer\", \"football\", \"basketball\"],\n",
    "}\n",
    "\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "http_body = httpbody_pb2.HttpBody(\n",
    "    data=json_data.encode(\"utf-8\"),\n",
    "    content_type=\"application/json\",\n",
    ")\n",
    "\n",
    "request = aiplatform_v1.RawPredictRequest(\n",
    "    endpoint=endpoint.resource_name,\n",
    "    http_body=http_body,\n",
    ")\n",
    "\n",
    "response = prediction_client.raw_predict(request)\n",
    "json.loads(response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete(force=True)\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts docker images delete --quiet --delete-tags {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\n",
    "!gcloud storage rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SDK_Custom_Predict_and_Handler_SDK_Integration.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
