{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the ðŸ¤— HuggingFace Hub to Vertex AI on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will go through the same steps as mentioned before in the following notebooks: `01-build-custom-inference-images.ipynb` and `02-upload-register-and-deploy-models.ipynb`, similarly to `03-from-hub-to-vertex-ai.ipynb` which is an end to end guide on how to deploy models from the HuggingFace Hub into Vertex AI on CPU, this notebook will only add replacements/modifications were needed in order to use the GPU.\n",
    "\n",
    "For a more detailed explanation on each step, please consider having a look at the notebooks mentioned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `gcloud` CLI needs to be installed and logged in the project that will be used. See the installation notes at https://cloud.google.com/sdk/docs/install\n",
    "\n",
    "* `docker` needs to be installed locally, and up and running, since it will be used to build the CPR images before pushing those to the container registry. See the installation notes at https://docs.docker.com/engine/install/\n",
    "\n",
    "* `google-cloud-aiplatform` Python library is required to programatically build the CPR image, to define the custom prediction code via a custom `Predictor`, to register and deploy the model to an endpoint in Vertex AI, and to run the online prediction on it.\n",
    "\n",
    "    `pip install google-cloud-aiplatform --upgrade`\n",
    "\n",
    "* `git lfs` needs to be installed for pulling / cloning models from the HuggingFace Hub. See the installation notes at https://git-lfs.com/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To successfully run the code below, you will need to be authenticated into your Google Cloud account and the following variable values must be set in advance:\n",
    "\n",
    "* `REGION` is the region where the resources will be hosted in.\n",
    "* `PROJECT_ID` is the identifier of the project in Google Cloud.\n",
    "* `REPOSITORY` is the directory where the Docker images will be uploaded to.\n",
    "* `IMAGE` is the name of the Docker image (without tag).\n",
    "* `TAG` is that tag of the Docker image.\n",
    "* `BUCKET_NAME` is the name of the bucket were the model will be / has been uploaded to.\n",
    "* `BUCKET_URI` is the full path to the `model.tar.gz` file in Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"europe-west4\"\n",
    "PROJECT_ID = \"huggingface-cloud\"\n",
    "REPOSITORY = \"custom-inference-gpu\"\n",
    "IMAGE = \"huggingface-pipeline-gpu\"\n",
    "TAG = \"py310-cu12.3-torch-2.2.0-transformers-4.38.1\"\n",
    "BUCKET_NAME = \"huggingface-cloud\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}/bart-large-mnli/model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note**: before choosing a region, you should ensure that the region has access to machines with GPU accelerators, otherwise, the deployment will fail as some zones won't have access to those i.e. `europe-west4` has but `europe-west9` hasn't. More information at https://cloud.google.com/vertex-ai/pricing#prediction-prices._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Prediction Routine (CPR) using ðŸ¤— `tranformers.pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some things to take into consideration before developing the `HuggingFacePredictor` that will be in charge of running the inference for the model above, as we need to ensure that it uses the GPU accelerator. Since we're using the `pipeline` method from `transformers`, we can simply rely on the `device_map` argument that has an `auto` option (powered by `accelerate`) that will automatically define the device mapping for the given model.\n",
    "\n",
    "For more information check [`01-build-custom-inference-images.ipynb`](./01-build-custom-inference-images.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir huggingface_predictor_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile huggingface_predictor_gpu/predictor.py\n",
    "import os\n",
    "import logging\n",
    "import tarfile\n",
    "from typing import Any, Dict\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "class HuggingFacePredictor(Predictor):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def load(self, artifacts_uri: str) -> None:\n",
    "        \"\"\"Loads the preprocessor and model artifacts.\"\"\"\n",
    "        logger.debug(f\"Downloading artifacts from {artifacts_uri}\")\n",
    "        prediction_utils.download_model_artifacts(artifacts_uri)\n",
    "        logger.debug(\"Artifacts successfully downloaded!\")\n",
    "        os.makedirs(\"./model\", exist_ok=True)\n",
    "        with tarfile.open(\"model.tar.gz\", \"r:gz\") as tar:\n",
    "            tar.extractall(path=\"./model\")\n",
    "        logger.debug(f\"HF_TASK value is {os.getenv('HF_TASK')}\")\n",
    "        self._pipeline = pipeline(os.getenv(\"HF_TASK\", None), model=\"./model\", device_map=\"auto\")\n",
    "        logger.debug(\"`pipeline` successfully loaded!\")\n",
    "        logger.debug(f\"`pipeline` is using device={self._pipeline.device}\")\n",
    "\n",
    "    def predict(self, instances: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        return self._pipeline(**instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that since we're using `device_map` from ðŸ¤— `accelerate`, we will need to also add `accelerate` as a dependency within the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile huggingface_predictor_gpu/requirements.txt\n",
    "torch==2.2.0\n",
    "transformers==4.38.1\n",
    "accelerate==0.27.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can already build the new Docker image with support for GPU inference, and we will use a custom image from the Docker Hub `alvarobartt/torch-gpu:py310-cu12.3-torch-2.2.0` which has been put together so that it's easier to use within Vertex AI, so that the user just needs to add the dependencies within `requirements.txt` without caring about the CUDA drivers and such.\n",
    "\n",
    "Anyway, everyone can create their own custom Docker image, but it should have the following:\n",
    "\n",
    "* `python` installed under the alias of `python`, not `python3`\n",
    "* `pip` installed under the alias of `pip`, not `pip3`\n",
    "* Needs to be built with `--platform=linux/amd64` to work on Vertex AI (if not using Linux already)\n",
    "\n",
    "_**Note**: I will soon add another tutorial on how to create custom Docker images and directly push those to Google's Container Registry without having to rely on `LocalModel.build_cpr_model`, in an easier and more flexible way._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "\n",
    "from huggingface_predictor_gpu.predictor import HuggingFacePredictor\n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    \"huggingface_predictor_gpu\",\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}:{TAG}\",\n",
    "    predictor=HuggingFacePredictor,\n",
    "    requirements_path=\"huggingface_predictor_gpu/requirements.txt\",\n",
    "    base_image=\"--platform=linux/amd64 alvarobartt/torch-gpu:py310-cu12.3-torch-2.2.0 AS build\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create custom-inference-gpu --repository-format=docker --location={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.push_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/alvarobartt/vertex-ai-huggingface/main/online-prediction/assets/docker-artifact-registry.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model from the ðŸ¤— Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to decide which model from the HuggingFace Hub we want to use, in this case, we will be using `facebook/bart-large-mnli` which is a zero-shot classification model.\n",
    "\n",
    "In order to do so, we will pull the model from the HuggingFace Hub using `git pull`, which requires `git lfs` to be installed in advanced, in order to also pull the large files from the repository.\n",
    "\n",
    "For more information check [`02-upload-register-and-deploy-models.ipynb`](./02-upload-register-and-deploy-models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/facebook/bart-large-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd bart-large-mnli/ && tar zcvf model.tar.gz --exclude flax_model.msgpack --exclude pytorch_model.bin --exclude rust_model.ot * && mv model.tar.gz ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "!gcloud config set storage/parallel_composite_upload_enabled True\n",
    "!gcloud storage cp model.tar.gz $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "!gcloud storage ls --recursive gs://{BUCKET_NAME}/bart-large-mnli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/alvarobartt/vertex-ai-huggingface/main/online-prediction/assets/model-cloud-storage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model in Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is uploaded to GCS and that the CPR image has been pushed to Google's Container Registry, then we can already register the model in Vertex AI.\n",
    "\n",
    "For more information check [`02-upload-register-and-deploy-models.ipynb`](./02-upload-register-and-deploy-models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d682d8388ec"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2738154345d5"
   },
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"bart-large-mnli\",\n",
    "    artifact_uri=\"gs://huggingface-cloud/bart-large-mnli\",\n",
    "    serving_container_image_uri=local_model.get_serving_container_spec().image_uri,\n",
    "    serving_container_environment_variables={\n",
    "        \"HF_TASK\": \"zero-shot-classification\",\n",
    "        # Optional env var so that `uvicorn` only runs the model in 1 worker\n",
    "        # \"VERTEX_CPR_WEB_CONCURRENCY\": 1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/alvarobartt/vertex-ai-huggingface/main/online-prediction/assets/model-registry-vertex-ai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note**: the `deploy` method will take a while ~15-20 minutes in order to deploy the model in Vertex AI as an endpoint._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since we want to run the inference on GPU, we need to ensure that the `machine_type` matches a machine with GPU, those being the ones from the G2 Series. Also, as already mentioned at the beginning of the notebook, some regions/zones may not have access to GPU accelerators, so double check that the region/zone that you're using has access to the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62cf66498a28"
   },
   "outputs": [],
   "source": [
    "endpoint = model.deploy(\n",
    "    machine_type=\"g2-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    accelerator_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/alvarobartt/vertex-ai-huggingface/main/online-prediction/assets/endpoint-deployment-vertex-ai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run online predictions on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can proceed to run the online predictions on Vertex AI using their Python client, which will basically send the requests to the running endpoint, and we will also be able to closely monitor it via Google Cloud Logging service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from google.api import httpbody_pb2\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "prediction_client = aiplatform_v1.PredictionServiceClient(\n",
    "    client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
    ")\n",
    "\n",
    "data = {\n",
    "    \"sequences\": \"Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\",\n",
    "    \"candidate_labels\": [\"mobile\", \"website\", \"billing\", \"account access\"],\n",
    "}\n",
    "\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "http_body = httpbody_pb2.HttpBody(\n",
    "    data=json_data.encode(\"utf-8\"),\n",
    "    content_type=\"application/json\",\n",
    ")\n",
    "\n",
    "request = aiplatform_v1.RawPredictRequest(\n",
    "    endpoint=endpoint.resource_name,\n",
    "    http_body=http_body,\n",
    ")\n",
    "\n",
    "response = prediction_client.raw_predict(request)\n",
    "json.loads(response.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can release the resources we've allocated / created using the following `delete` methods over both the `endpoint` and the `model` variables in Vertex AI, and also the following `gcloud` CLI commands to remove both the Docker image from the Container Registry and the HuggingFace model from the Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete(force=True)\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts docker images delete --quiet --delete-tags {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\n",
    "!gcloud storage rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SDK_Custom_Predict_and_Handler_SDK_Integration.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
