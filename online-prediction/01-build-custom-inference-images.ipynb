{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1210e7-a6dc-4aaa-ad05-71a4021e5bef",
   "metadata": {},
   "source": [
    "# Build images for custom inference for ðŸ¤— HuggingFace models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d2dc2a-b14d-4d7f-a2b2-08de387137c4",
   "metadata": {},
   "source": [
    "In order to deploy HuggingFace models in Vertex AI we will need to make use of the Custom Prediction Routines (CPR) which are Docker images built with custom inference code and requirements (among other optional stuff).\n",
    "\n",
    "These images need to be built for HuggingFace's libraries i.e. `tranformers`, `diffusers` and much more; as there are no official Docker images yet for HuggingFace, while those are available for `sklearn`, `AutoML` and `XGBoost` models.\n",
    "\n",
    "In this tutorial we will show how to use the Python library `google-cloud-aiplatform` to build and register new CPRs in Google's Container Registry, for its later inference usage within Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b3400-2a4e-4a4c-a2b2-6038a8c8d13f",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d124f-3484-4961-b3a6-c6e8555f3665",
   "metadata": {},
   "source": [
    "* `gcloud` CLI needs to be installed and logged in the project that will be used to push the Docker images. See the installation notes at https://cloud.google.com/sdk/docs/install\n",
    "\n",
    "* `docker` needs to be installed locally, and up and running, since it will be used to build the CPR images before pushing those to the container registry. See the installation notes at https://docs.docker.com/engine/install/\n",
    "\n",
    "* `google-cloud-aiplatform` Python library is required to programatically build the CPR image and to define the custom prediction code via a custom `Predictor`.\n",
    "\n",
    "    `pip install google-cloud-aiplatform --upgrade`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9d0f2-70aa-4c9c-a09c-923399a00f29",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf982f7-305e-4951-abc2-439accea80c4",
   "metadata": {},
   "source": [
    "To successfully run the code below, you will need to be authenticated into your Google Cloud account and the following variable values must be set in advance:\n",
    "\n",
    "* `REGION` is the region where the resources will be hosted in.\n",
    "* `PROJECT_ID` is the identifier of the project in Google Cloud.\n",
    "* `REPOSITORY` is the directory where the Docker images will be uploaded to.\n",
    "* `IMAGE` is the name of the Docker image (without tag).\n",
    "* `TAG` is that tag of the Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d929a-aa2e-464a-904d-30d349340591",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"europe-west9\"\n",
    "PROJECT_ID = \"huggingface-cloud\"\n",
    "REPOSITORY = \"custom-inference\"\n",
    "IMAGE = \"huggingface-pipeline\"\n",
    "TAG = \"py310-cpu-torch-2.2.0-transformers-4.37.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b1686-95ca-4cd2-8f76-b1d53f106404",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a93acb3-fd28-4f81-a3d9-36492029c712",
   "metadata": {},
   "source": [
    "_**Note**: the following is a CPU-only version for custom inference, to use GPU for inference, the `device_map` should be defined (ideally with `auto` as the value), `accelerate` should be included to `requirements.txt`, and a CUDA image should be used when building the CPR via `build_cpr_model`._\n",
    "\n",
    "Check the end to end notebook suitable for GPU inference at [`04-from-hub-to-vertex-ai-gpu.ipynb`](./04-from-hub-to-vertex-ai-gpu.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f374e4-8d20-46bc-aade-a27ecc3b3b1a",
   "metadata": {},
   "source": [
    "## Custom inference code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec09db-920b-4aac-a6da-4bcd058de8e8",
   "metadata": {},
   "source": [
    "In order to successfully run the inference over a HuggingFace model, we will need to define a custom class inheriting from the `Predictor` class from `google-cloud-aiplatform`.\n",
    "\n",
    "To run the inference we will use the `pipeline` method from ðŸ¤— `transformers`, which will be loaded as part of the `Predictor.load` method, controlled by the environment variable `HF_TASK`; then the `pipeline` will run within the `predict` method and will generate the output as a Python dict.\n",
    "\n",
    "Alternatively, we could also implement ourselves the code rather than relying on `pipeline` if our model needs to suit specific needs, while the `pipeline` controlled via the `HF_TASK` environment variable gives the image flexibility as we can build one image and make it work for different models and tasks (as long as those don't have extra requirements), and custom images with too specific codes may only perform well under certain scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240dffb-c281-4741-aa54-f9bb67b9d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir huggingface_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a237d6ff-4f97-4f8e-a2ed-63e3c162afd3",
   "metadata": {},
   "source": [
    "_Note that we will use the magic method `%%writefile` to write the Python code of the `HuggingFacePredictor` into the `predictor.py` file, which won't import the code within the Jupyter Notebook, so we will need to load it afterwards using `%load`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec7465-be6b-492a-bcde-5df91eb10109",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile huggingface_predictor/predictor.py\n",
    "import os\n",
    "import logging\n",
    "import tarfile\n",
    "from typing import Any, Dict\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "class HuggingFacePredictor(Predictor):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def load(self, artifacts_uri: str) -> None:\n",
    "        \"\"\"Loads the preprocessor and model artifacts.\"\"\"\n",
    "        logger.debug(f\"Downloading artifacts from {artifacts_uri}\")\n",
    "        prediction_utils.download_model_artifacts(artifacts_uri)\n",
    "        logger.debug(f\"Artifacts successfully downloaded!\")\n",
    "        os.makedirs(\"./model\", exist_ok=True)\n",
    "        with tarfile.open(\"model.tar.gz\", \"r:gz\") as tar:\n",
    "            tar.extractall(path=\"./model\")\n",
    "        logger.debug(f\"HF_TASK value is {os.getenv('HF_TASK')}\")\n",
    "        self._pipeline = pipeline(os.getenv(\"HF_TASK\", None), model=\"./model\")\n",
    "        logger.debug(f\"`pipeline` successfully loaded!\")\n",
    "\n",
    "    def predict(self, instances: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        return self._pipeline(**instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e588b-7319-481a-9576-dee56c18474b",
   "metadata": {},
   "source": [
    "Besides the `Predictor` we can alternatively add a `requirements.txt` file containing the requirements needed to run the code snippet above, which will be installed as part of the Custom Prediction Routine (CPR), that will build a Vertex AI compatible Docker image automatically including both `predictor.py` and `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e9a5da-25fc-43cf-988e-da5280c70779",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile huggingface_predictor/requirements.txt\n",
    "torch==2.2.0\n",
    "transformers==4.37.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed9207-d092-43ae-845d-c4eb0cfadcab",
   "metadata": {},
   "source": [
    "## Build Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca09fa-534f-4734-a69f-c209c8d9951a",
   "metadata": {},
   "source": [
    "Before building the image we will need to create the Docker repository in Google Artifact Registry, otherwise it will fail not when building the image but when trying to push it, meaning that we would need to re-run the build, so make sure that the repository exists in advance or create it with `gcloud` as it follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d86d1-a0b2-41c7-861c-24454935c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create custom-inference --repository-format=docker --location={REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb1ead9-6f40-4e28-8b47-21015b500a8b",
   "metadata": {},
   "source": [
    "Once we've ensured that the Docker repository exists and that we've packaged the code into the `huggingface_predictor` directory (containing both `predictor.py` and `requirements.txt`), we will build the Custom Prediction Routine (CPR) which will create and build the Docker image in the Google Container Registry.\n",
    "\n",
    "In this case we need to define the following args:\n",
    "* `src_dir` is the path to the local directory including the required files (will be copied into the image)\n",
    "* `output_image_uri` is the URI where the Docker image will be pushed to in Google Cloud\n",
    "* `predictor` is the class instance that inherits from the `Predictor` i.e. `HuggingFacePredictor`\n",
    "* `requirements_path` is the path to the `requirements.txt` file\n",
    "* `base_image` is the base image that will be defined within the Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cbd437-4b4e-4609-894e-b451e5658b58",
   "metadata": {},
   "source": [
    "_Note that `docker` needs to be installed in advanced and running, since it will be internally used to build the image, and it may take a while._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fed392-5311-4840-9a19-715b543b5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "\n",
    "from huggingface_predictor.predictor import HuggingFacePredictor\n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    \"huggingface_predictor\",\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}:{TAG}\",\n",
    "    predictor=HuggingFacePredictor,\n",
    "    requirements_path=\"huggingface_predictor/requirements.txt\",\n",
    "    base_image=\"--platform=linux/amd64 python:3.10 AS build\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681fc0c1-b3fb-4abb-9d42-d3d65d739f83",
   "metadata": {},
   "source": [
    "## Push Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81912453-c376-42e2-a472-5f6a6921c00c",
   "metadata": {},
   "source": [
    "Once the image has been built, we can push it to the Google Container Registry via the `push_image` method of the `LocalModel`. But before pushing the image we will need to ensure that the container registry is configured via the `gcloud auth configure-docker` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bbe25-62d3-4d41-8b35-c8734fad47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d6eae-0d62-4e98-8c6b-d84ec43771c1",
   "metadata": {},
   "source": [
    "Then we can call the `push_image` method that will internally call `docker push` to the container registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5216ed4-ccbf-4c11-bd03-1082d0ae22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.push_image()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc4265d7-8e0b-4aff-9183-83fcf3d1e6f8",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/alvarobartt/vertex-ai-huggingface/main/online-prediction/assets/docker-artifact-registry.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
