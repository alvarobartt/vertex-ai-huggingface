{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc03b2f2-001b-4c39-864d-b01bf4cec602",
   "metadata": {},
   "source": [
    "# Upload, register and deploy models from the ðŸ¤— Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe85d3-a6c3-4e88-a7d0-fe5e5f9b9852",
   "metadata": {},
   "source": [
    "In order to deploy a model in Vertex AI, we will first need to upload it to a Google Cloud Storage (GCS) bucket, and then register it in Vertex AI; meaning that we cannot deploy straight from the HuggingFace Hub, but using GCS as the intermediate storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fff362-11f9-4d4d-bdcb-bc06d743a37a",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0426d78-bdfb-483d-8fa5-b9cd6c9eca8b",
   "metadata": {},
   "source": [
    "* `gcloud` CLI needs to be installed and logged in the project that will be used. See the installation notes at https://cloud.google.com/sdk/docs/install\n",
    "\n",
    "* `google-cloud-aiplatform` Python library is required to register the model and deploy it as an endpoint in Vertex AI.\n",
    "\n",
    "    `pip install google-cloud-aiplatform --upgrade`\n",
    "\n",
    "* `git lfs` needs to be installed for pulling / cloning models from the HuggingFace Hub. See the installation notes at https://git-lfs.com/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa6442-2ba5-4899-8e59-0e24223b90b5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e0887-a873-4af1-8f5c-235a0b359ac8",
   "metadata": {},
   "source": [
    "To successfully run the code below, you will need to be authenticated into your Google Cloud account and the following variable values must be set in advance:\n",
    "\n",
    "* `REGION` is the region where the resources will be hosted in.\n",
    "* `PROJECT_ID` is the identifier of the project in Google Cloud.\n",
    "* `REPOSITORY` is the directory where the Docker images will be uploaded to.\n",
    "* `IMAGE` is the name of the Docker image (without tag).\n",
    "* `TAG` is that tag of the Docker image.\n",
    "* `BUCKET_NAME` is the name of the bucket were the model will be / has been uploaded to.\n",
    "* `BUCKET_URI` is the full path to the `model.tar.gz` file in Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a8b17-4a27-45d2-9955-3e4d0672c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"europe-west9\"\n",
    "PROJECT_ID = \"huggingface-cloud\"\n",
    "REPOSITORY = \"custom-inference\"\n",
    "IMAGE = \"huggingface-pipeline\"\n",
    "TAG = \"py310-cpu-torch-2.2.0-transformers-4.38.1\"\n",
    "SERVING_CONTAINER_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}:{TAG}\"\n",
    "BUCKET_NAME = \"huggingface-cloud\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}/bart-large-mnli/model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e1243-fdb8-4754-8b27-ade13d4f1b52",
   "metadata": {},
   "source": [
    "## Upload model from the ðŸ¤— Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1d688-44de-4db9-9bc2-67eba21fae2b",
   "metadata": {},
   "source": [
    "First we need to decide which model from the HuggingFace Hub we want to use, in this case, we will be using `facebook/bart-large-mnli` which is a zero-shot classification model, but could be any model in the Hub.\n",
    "\n",
    "In order to do so, we will pull the model from the HuggingFace Hub using `git pull`, which requires `git lfs` to be installed in advanced, in order to also pull the large files from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77aeb4-a4a9-4086-a5a5-beff8b5f3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/facebook/bart-large-mnli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3e90d-930f-416f-b8b0-b979531ed937",
   "metadata": {},
   "source": [
    "Once we clone it, we will need to decide which files we want to package into `model.tar.gz` and which models we want to leave outside i.e. the weights in other frameworks instead of the one we want to use e.g. `torch`, and any other file that may be part of the model repository that's not needed to load the model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6d294-5feb-4c9c-8d5c-a9c7e94f1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd bart-large-mnli/ && tar zcvf model.tar.gz --exclude flax_model.msgpack --exclude pytorch_model.bin --exclude rust_model.ot * && mv model.tar.gz ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf230f-d372-401c-bc72-233801c2d79e",
   "metadata": {},
   "source": [
    "Once we've packaged all the required files into `model.tar.gz`, we can upload it to Google Cloud Storage, so that the URI pointing to that file will be later on provided to Vertex AI to register the model from that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b145c0-e6f2-4466-b0ca-1ef88c8606de",
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "!gcloud config set storage/parallel_composite_upload_enabled True\n",
    "!gcloud storage cp model.tar.gz $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97cf23-cffc-46d1-8d70-6771bea50e0f",
   "metadata": {},
   "source": [
    "Optionally, we can `gcloud storage ls` to ensure that the `model.tar.gz` file has been indeed uploaded to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a7ebe-478b-45ad-9b9d-138876feff39",
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "!gcloud storage ls --recursive gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2753e46d-3659-4a6c-9a68-1c97277edbec",
   "metadata": {},
   "source": [
    "## Register model in Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001de1b4-a85e-45f4-9c5a-ad3f0bdfb5fa",
   "metadata": {},
   "source": [
    "Once the model is uploaded to GCS, we can already register it in Vertex AI, but to do so we will also need to specify the container that will run that model in advance.\n",
    "\n",
    "It could be any container with `python` and `pip` installed, and the required CPR requirements met, so that the endpoint can be deployed normally, otherwise the deployment will fail.\n",
    "\n",
    "For more information check [`01-build-custom-inference-images.ipynb`](./01-build-custom-inference-images.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e49e1b-7e68-46b1-b828-96d742591bca",
   "metadata": {
    "id": "8d682d8388ec"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1dcc3d-9174-42c9-8e5b-ced0ecdaad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25184f6-6692-471b-9ba7-5d38470b3d32",
   "metadata": {},
   "source": [
    "So we will be using `google-cloud-aiplatform` to register a model from GCS into Vertex AI, which in this case will match the model previously uploaded to GCS.\n",
    "\n",
    "_**Note**: that all the `serving_*` arguments of the classmethod `upload` from `aiplatform.Model` refer to the container that will be pulled from the container registry and used during inference when deploying the endpoint, which must have been created in advance, as explained in `01-build-custom-inference-images.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d948cf-cf8e-4225-b315-72c8bc744aa0",
   "metadata": {
    "id": "2738154345d5"
   },
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"bart-large-mnli\",\n",
    "    artifact_uri=\"gs://huggingface-cloud/bart-large-mnli\",\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    serving_container_environment_variables={\n",
    "        \"HF_TASK\": \"zero-shot-classification\",\n",
    "        # Optional env var so that `uvicorn` only runs the model in 1 worker\n",
    "        # \"VERTEX_CPR_WEB_CONCURRENCY\": 1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f4dea-dddd-4c14-9f7a-4fa407aa7c2a",
   "metadata": {},
   "source": [
    "_**Note**: the `deploy` method will take a while ~15-20 minutes in order to deploy the model in Vertex AI as an endpoint._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47b05a-be73-4b62-bff9-38b061f2fa95",
   "metadata": {},
   "source": [
    "## Deploy endpoint in Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bc372-7de3-49e3-be6f-78a14be24498",
   "metadata": {},
   "source": [
    "Finally, we can use the `aiplatform.Model` object returned before when the `upload` class method was called, to call the `deploy` method that will deploy an endpoint using FastAPI (unless the handler in the CPR was overwritten) running in a machine matching the `machine_type` argument.\n",
    "\n",
    "In this case we will only define the `machine_type` arg, and we will use the `e2-standard-4` which is a VM from Compute Engine with 4 vCPUs and 16GiB of RAM.\n",
    "\n",
    "To see all the possible args of `deploy` see the source code at [`google.cloud.aiplatform.models`](https://github.com/googleapis/python-aiplatform/blob/f249353b918823b35495b295a75a90528ad652c0/google/cloud/aiplatform/models.py#L3418)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7329f2-eb35-40ec-9e5a-388ed052fe6f",
   "metadata": {
    "id": "62cf66498a28"
   },
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"e2-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabf289-48f3-4174-9067-eb2d506ea966",
   "metadata": {},
   "source": [
    "## Online predictions in Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1598b4-3f88-4de8-a585-9de104986e48",
   "metadata": {},
   "source": [
    "Then, once the Vertex AI endpoint is running, we can try it out using the `PredictionServiceClient` from `google-cloud-aiplatform` to send an HTTP request to the `bart-base-mnli` model running the `zero-shot-classification` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b335f4-ea96-4c11-a681-4bc98e7540f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from google.api import httpbody_pb2\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "prediction_client = aiplatform_v1.PredictionServiceClient(\n",
    "    client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
    ")\n",
    "\n",
    "data = {\n",
    "    \"sequences\": \"Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\",\n",
    "    \"candidate_labels\": [\"mobile\", \"website\", \"billing\", \"account access\"],\n",
    "}\n",
    "\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "http_body = httpbody_pb2.HttpBody(\n",
    "    data=json_data.encode(\"utf-8\"),\n",
    "    content_type=\"application/json\",\n",
    ")\n",
    "\n",
    "request = aiplatform_v1.RawPredictRequest(\n",
    "    endpoint=endpoint.resource_name,\n",
    "    http_body=http_body,\n",
    ")\n",
    "\n",
    "response = prediction_client.raw_predict(request)\n",
    "json.loads(response.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabfc951-6d6d-420c-9044-cf79c8028ef7",
   "metadata": {},
   "source": [
    "## Resource clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85ebec-732b-4446-a4e5-841d4dcf659d",
   "metadata": {},
   "source": [
    "Finally, we clean up the resources used i.e. the Vertex AI endpoint and the model. In this case, we clean the resources up because we are no longer going to use those, but alternatively we could leave those running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b628def-8d82-4733-9796-287dd2cb583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete(force=True)\n",
    "model.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
